{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniproject ML4 Snake\n",
    "\n",
    "# 1. and 2. single- and turn-based multi-player snake\n",
    "\n",
    "It's much harder to get good results with RL compared to other areas of ML. So always **start your RL as simple as possible. Once you've got good results you can slowly add complexity/reaslism.** So rather than starting directly with multi-player snake, let's start with single-player snake.\n",
    "\n",
    "OpenAI Gym is the facto standard as RL platform, so it's wise to use a snake environment based on OpenAI Gym. Unfortunately, snake is not part of the environments officially supported by OpenAI Gym, so we use **a modified version of** [this repo](https://github.com/grantsrb/Gym-Snake), providing A single- and multi-snake environment and nice graphics. The adapted environment can be found in the zip on BlackBoard->ML4->Miniproject, in the folder `gym_snake`.\n",
    "\n",
    "\n",
    "## Understanding the OpenAI Gym platform\n",
    "\n",
    "The nice thing about OpenAI Gym is that it separates agent and environment via a well-defined interface for the environment. The important methods on the interface of the environment are:\n",
    "* `env.reset()` creates a new, clean environment; snake: initializes a new snake game\n",
    "* `obs, reward, done, info = env.step(action)` performs an agent action and immediately returns:\n",
    "  * `obs`: the changed environment state, called the observation\n",
    "  * `reward`: the reward, resulting from the action\n",
    "  * `done`: whether the game has finished\n",
    "  * `info`: environment-specific additional info\n",
    "  Based on this information the agent decides what the next action will be. Tha agent continues this loop until `done` is true.\n",
    "* `env.render()` draws the current state of the environment\n",
    "\n",
    "The idea is that you create the agent and leave the environment untouched. However in reality you might want to change the environment. An example is the reward function, which is part of the environment.\n",
    "\n",
    "\n",
    "## Pixel-based vs. coordinate-based environment state representation\n",
    "\n",
    "The original snake environment of Grantsrb uses a pixel-based environment state representation, rather than a coordinate-based environment state representation. This hugely increases the state size and therefore makes learning slower and harder. This goes against the start-as-simple-as-possible principle. \n",
    "\n",
    "A method to perform dimensionality reduction of the state space is using a CNN. The CNN handles the pixel-based state and reduces its dimensionality before passing it to the RL algorithm. But again, a CNN component must be tuned and this adds complexity.\n",
    "\n",
    "To overcome this, the original code has been slightly adapted: the code keeps using a pixel-based environment state internally, but just before passing it to the agent, it transforms it to coordinate-based environment state. This is really a hack, as it is very inefficient to transform from pixel-based to coordinate-based with every world-tick, but to completely change the internal state representation would have been quite some work. Note that the change from pixel-based to coordinate-based reduced the state size by a factor of 100 (every cell has 10x10 pixels), but not the state space size (the number of grid cells is unchanged).\n",
    "\n",
    "Once you get good results with the coordinate-based environment you might want to try the pixel-based environment. This is where RL shows its magic. To use the pixel-based environment, set `self.coordinate_based = False` in the file `snake_env.py`.\n",
    "\n",
    "\n",
    "## Installation of the snake environment\n",
    "\n",
    "* create a new conda environment running python 3.9: `conda create -n py39 python=3.9`\n",
    "* `conda activate py39`\n",
    "* unpack the zip from Blackboard->ML4->Miniproject\n",
    "* go the folder of the zip and put your RL code there\n",
    "* If you use Jupyter notebook for your agent and you happen to change somethin in the environment, you need to **restart the kernel** to see environment changes.\n",
    "\n",
    "This method is not the normal way of working. In the normal way of working you use `pip install -e ./` to install the snake package **and** register the snake environment with OpenAI Gym. The disadvantage of the normal way of working is that every time you change the environment, you need to re-register to see the environment changes. This is quite a hassle and easily forgotten. Advantage of the above method is that code changes in the environment are available directly. \n",
    "\n",
    "\n",
    "## Running snake\n",
    "\n",
    "The rendering of the snake does not work well in a Jupyter notebook. You can use a Jupyter notebook to develop your solution, but to get correct rendering, run the code via the command prompt or via an IDE. Probably it's better to not use Jupyter notebook at all for your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example agent for 1. single-player snake\n",
    "\n",
    "See the [README](https://github.com/grantsrb/Gym-Snake) for explanation about the snake environment. \n",
    "\n",
    "I didn't go through the hassle of registering it with OpenAI Gym, so you use it as any other python class. This means to create an environment: `env = SnakeEnv(grid_size=[6, 6], snake_size=2)`.\n",
    "\n",
    "Below a pretty dumb example agent for single-player snake that gives you an idea how to start your agent. Don't put your agent code inside the `gym_snake` folder, but just one level above it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example agent for single-player snake\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import logging\n",
    "import gym\n",
    "#import gym_snake  # don't use the registered snake\n",
    "from gym_snake.envs.snake_env import SnakeEnv\n",
    "\n",
    "log = logging.getLogger(\"miniproject_snake\")\n",
    "log.setLevel(logging.INFO)\n",
    "log.addHandler(logging.StreamHandler())\n",
    "\n",
    "# actions\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "\n",
    "env = SnakeEnv(grid_size=[6, 6], snake_size=2)\n",
    "obs = env.reset()  # construct instance of game\n",
    "done = False\n",
    "log.info(\"start game\")\n",
    "for i in range(24):\n",
    "    if not done:\n",
    "        env.render()\n",
    "        obs, reward, done, info = env.step(i%4)  # pass action to step()\n",
    "        log.info(\"reward: %.0f\", reward)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution for single-player snake\n",
    "\n",
    "For the solution I cheated a bit. Instead of implementing an own solution, I used the RL library Stable Baselines. The algorithm chosen is DQN, deep Q-learning. The default setting is that in fact DDQN, double deep Q-learning, is used.\n",
    "\n",
    "### Installation of the stable-baselines RL library\n",
    "\n",
    "* `conda activate py39`\n",
    "* `conda install -c pytorch pytorch`\n",
    "* `conda install pip`\n",
    "* `<location of anaconda>\\anaconda\\envs\\py39\\pip install stable-baselines3` (to be sure to use the correct pip binary)\n",
    "* `<location of anaconda>\\anaconda\\envs\\py39\\pip install tensorboard`\n",
    "    \n",
    "### Environment code changes needed for Stable Baselines\n",
    "\n",
    "Below the environment code changes needed for Stable Baselines. The adapted environment can be found in the zip in the folder `gym_snake`.\n",
    "\n",
    "**In snake_env.py:**\n",
    "\n",
    "Replace Discrete(4) by spaces.Discrete(4), so:\n",
    "\n",
    "`self.action_space = spaces.Discrete(4)`\n",
    "\n",
    "Add this line for coordinate-based state space:\n",
    "\n",
    "`self.observation_space = spaces.Box(low=0, high=3, shape=(self.grid_size[1], self.grid_size[0]), dtype=np.uint8)`\n",
    "\n",
    "Or alternatively, add this line for pixel-based state space:\n",
    "\n",
    "`self.observation_space = spaces.Box(low=0, high=255, shape=(self.grid_size[1] * self.unit_size, self.grid_size[0] * self.unit_size, 3), dtype=np.uint8)`\n",
    "\n",
    "**In controller.py:**\n",
    "\n",
    "Replace `if type(directions) == type(int()):` by:\n",
    "\n",
    "`if type(directions) == type(np.int64()) or type(directions) == type(int()):`\n",
    "\n",
    "\n",
    "### Discussion\n",
    "\n",
    "The learned model is saved. With `training = False` the learned model is loaded from disk and used by the agent.\n",
    "\n",
    "**Assessing learning behavior**\n",
    "Note that during training, after every 100 episodes (== games), the *mean 100 episode reward* is shown. This number should be slowly increasing as training proceeds, indicating that the agent is learning.\n",
    "\n",
    "Note that exploring starts at 100% and reduces to about 2%.\n",
    "\n",
    "Another way of evaluating the learning behavior is to look at the tensorboard logs. \n",
    "* Install Tensorboard by opening a anaconda prompt and typing `conda install tensorboard`.\n",
    "* In the anaconda command prompt, go to the folder where your project resides, using `cd`.\n",
    "* Start Tensorboard by typing `tensorboard --logdir tensorboard_logs/` in the anaconda command prompt.\n",
    " \n",
    "**Snake behavior**\n",
    "Note that the reward function does not give the snake an incentive to be efficient in capturing food. Only the discount factor helps the snake te become efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Solution for single-player snake using RL library Stable Baselines\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import logging\n",
    "import gym\n",
    "#import gym_snake  # don't use the registered snake\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "from gym_snake.envs.snake_env import SnakeEnv\n",
    "\n",
    "\n",
    "log = logging.getLogger(\"miniproject_snake\")\n",
    "log.setLevel(logging.INFO)\n",
    "log.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "'''\n",
    "# loading the environment if you've installed and registered it\n",
    "env = gym.make(\"snake-v0\")\n",
    "env = DummyVecEnv([lambda: env])\n",
    "'''\n",
    "\n",
    "'''\n",
    "# loading the environment locally (without registering); advantage is that you change the environment \n",
    "# without the need for reinstalling and reregistering\n",
    "# some algorithms require a vectorizes environment\n",
    "env = DummyVecEnv([lambda: SnakeEnv(grid_size=[6, 6], snake_size=2, random_init=False)])\n",
    "env.envs[0].grid_size = [6, 6]\n",
    "env.envs[0].snake_size = 2\n",
    "env.envs[0].random_init = False\n",
    "'''\n",
    "\n",
    "#env = SnakeEnv(grid_size=[6, 6], snake_size=2, random_init=False)\n",
    "env = SnakeEnv(grid_size=[6, 6], snake_size=2)\n",
    "obs = env.reset()  # construct instance of game\n",
    "\n",
    "'''\n",
    "print(\"grid_size\", env.grid_size)\n",
    "print(\"unit_size\", env.unit_size)\n",
    "print(\"unit_gap\", env.unit_gap)\n",
    "print(\"snake_size\", env.snake_size)\n",
    "print(\"n_snakes\", env.n_snakes)\n",
    "print(\"n_foods\", env.n_foods)\n",
    "snakes_array = env.controller.snakes\n",
    "snake_object1 = snakes_array[0]\n",
    "'''\n",
    "\n",
    "'''\n",
    "# get zoom-able & resize-able notebook, if you want to work interactively. BUG: only one zoomable notebook can be active. \n",
    "# If you don’t deactivate (end interaction to) it, you can’t draw another and get weird bugs in the following cells. \n",
    "# This is really bad if you have loops that generate plots in your code…\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "env.render()\n",
    "env.render()\n",
    "'''\n",
    "\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "\n",
    "training = True\n",
    "if training:\n",
    "    model = DQN(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"tensorboard_logs/snake_dqn_agent/\")\n",
    "    #model = DQN(\"MlpPolicy\", env, verbose=1, learning_rate=0.0005, gamma=0.99, policy_kwargs=dict(layers=[64, 64]), tensorboard_log=\"tensorboard_logs/snake_dqn_agent/\")\n",
    "\n",
    "    model.learn(total_timesteps=10000, reset_num_timesteps=False)\n",
    "    model.save(\"learned_models/snake_dqn_agent\")\n",
    "else:\n",
    "    #del model # remove to demonstrate saving and loading\n",
    "    model = DQN.load(\"learned_models/snake_dqn_agent\")\n",
    "\n",
    "log.info(\"finished training, now use the model and render the env\")\n",
    "obs = env.reset()\n",
    "done = False\n",
    "log.info(\"start game\")\n",
    "while not done:\n",
    "    env.render()\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    #log.info(\"reward: %.0f\", reward)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example agent for 2. turn-based multi-player snake\n",
    "\n",
    "Normally, in multi-player snake, all snakes move one step every world-tick. However this poses a problem, as most RL libraries don't have multi-agent support. A relatively easy way out is to use multi-player snake in a turn-based way. Turn-based means that at every world-tick only one snake advances and get a reward. The original snake environment has been adapted in such a way that it **only supports turn-based multi-player snake**. If you want to use the *normal* multi-player snake, use the environment from the original repo.\n",
    "\n",
    "To use the environment for turn-based multi-player snake:\n",
    "* declare an environment with multiple snakes\n",
    "* pass a single action `DOWN` and receive a single reward `1`. This is for the single snake that moved (the others didn't move)\n",
    "\n",
    "The [README](https://github.com/grantsrb/Gym-Snake) describes that you should use the `snake_extrahard_env.py` environment for the multi-player snake. This is not the case. **You can use `snake_env.py` for turn-based multi-player snake**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example agent for *turn-based* multi-player snake\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import logging\n",
    "import gym\n",
    "#import gym_snake  # don't use the registered snake\n",
    "from gym_snake.envs.snake_env import SnakeEnv\n",
    "\n",
    "log = logging.getLogger(\"miniproject_snake\")\n",
    "log.setLevel(logging.INFO)\n",
    "log.addHandler(logging.StreamHandler())\n",
    "\n",
    "NOMOVE = -1\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "\n",
    "#env = SnakeEnv(grid_size=[12, 12], snake_size=2, n_snakes=3, n_foods=3)\n",
    "env = SnakeEnv(grid_size=[9, 9], snake_size=2, n_snakes=2, n_foods=2)\n",
    "obs = env.reset()  # construct instance of game\n",
    "done = False\n",
    "log.info(\"start game\")\n",
    "for i in range(30):\n",
    "    if not done:\n",
    "        env.render()\n",
    "        #action = [DOWN, DOWN]  # *normal* multi-player snake: all snakes move at the same time and you receive a list of rewards\n",
    "        action = DOWN  # turn-based multi-player snake: one snake moves, other snakes don't move\n",
    "        obs, reward, done, info = env.step(action)  # reward is for the snake that has moved\n",
    "        log.info(\"reward: %.0f\", reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution for turn-based multi-player snake\n",
    "\n",
    "As for single-player snake, DQN from the RL library StableBaselines has been used. \n",
    "\n",
    "### Discussion\n",
    "    \n",
    "As for single-player snake, please verify that during training the *mean 100 episode reward* slowly increases as training proceeds, indicating that the agent is learning. Also look at the tensorboard logs to assess learning behavior.\n",
    "\n",
    "100 times more learning needed to get any decent learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for turn-based multi-player snake using RL library Stable Baselines\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import logging\n",
    "import gym\n",
    "#import gym_snake  # don't use the registered snake\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "from gym_snake.envs.snake_env import SnakeEnv\n",
    "\n",
    "log = logging.getLogger(\"miniproject_snake\")\n",
    "log.setLevel(logging.INFO)\n",
    "log.addHandler(logging.StreamHandler())\n",
    "\n",
    "env = SnakeEnv(grid_size=[9, 9], snake_size=2, n_snakes=2, n_foods=2)\n",
    "obs = env.reset()\n",
    "\n",
    "training = True\n",
    "if training:\n",
    "    model = DQN(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"tensorboard_logs/multisnake_dqn_agent/\")\n",
    "\n",
    "    model.learn(total_timesteps=100000, reset_num_timesteps=False)\n",
    "    model.save(\"learned_models/multisnake_dqn_agent\")\n",
    "else:\n",
    "    model = DQN.load(\"learned_models/multisnake_dqn_agent\")\n",
    "\n",
    "log.info(\"finished training, now use the model and render the env\")\n",
    "obs = env.reset()\n",
    "done = False\n",
    "log.info(\"start game\")\n",
    "while not done:\n",
    "    env.render()\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    #log.info(\"reward: %.0f\", reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. multi-player snake (cygni)\n",
    "\n",
    "## Installing & running the snake client\n",
    "\n",
    "**Installing the environment for the snake client**\n",
    "* `conda create -n py37 python=3.7` (the snake client requires python 3.7)\n",
    "* `conda activate py37`\n",
    "* `conda install numpy`\n",
    "* `conda install colorlog`\n",
    "* `pip install autobahn` (not available in conda)\n",
    "* download [this](https://github.com/cygni/snakebot-client-python/tree/master/client) folder\n",
    "\n",
    "**Running the client in training mode**\n",
    "* go the snake client folder\n",
    "* `python client/client.py -r snakebot.avans-informatica-breda.nl -p 8080 -l info`\n",
    "* Your bot plays against 4 randomly chosen bots\n",
    "* the one-before-last line of output contains the URL where the game can be replayed\n",
    "\n",
    "**Running the client in tournament mode**\n",
    "* go to http://snakebot.avans-informatica-breda.nl:8090/#/?_k=ynfagb\n",
    "* login: emil/lime\n",
    "* click “tournament” menu and create a tournament, e.g. “ercotournament”\n",
    "* `python client/client.py -r snakebot.avans-informatica-breda.nl -p 8080 -l info -v tournament`\n",
    "* add multiple bots in the same way, each with a different name\n",
    "* click “start tournament”\n",
    "* click “go to game” (the game is started and played very quickly)\n",
    "* some seconds later the GUI shows the game in slow mo\n",
    "* tournament options can be changed\n",
    "\n",
    "\n",
    "## The python client\n",
    "\n",
    "**Server logic**\n",
    "* turn-based server: all snakes are called once every world tick\n",
    "* server calls `get_next_move(self, game_map)` on every client\n",
    "* client has 250ms to respond (no timely response means no turn -> snake continues in same direction)\n",
    "* so as a client you perform a turn, and in the next invocation of `get_next_move(self, game_map)` you receive the updated map with which you can decide on the reward. This will take some programming effort.\n",
    "\n",
    "\n",
    "**Client logic**\n",
    "* pass `-l debug` instead of `-l info` to get more info about what's happening.\n",
    "* `snake.py` contains the actual snake logic. `snake.py`is passive; it's `client.py` that invokes methods on the `snake.py`.\n",
    "* `util.py` utility methods, for example to translate position to coordinates and vice versa\n",
    "* `client.py` creates the snake object and communicates with the server. No need to change code here.\n",
    "* `messages.py` contains definitions for communication with the server. No need to change code here.\n",
    "\n",
    "Note that as the full game map is passed to the snake with every invocation of `get_next_move(self, game_map)` the snake has full knowledge of the world!\n",
    "\n",
    "Let's have a look at the [code](https://github.com/cygni/snakebot-client-python/tree/master/client).\n",
    "\n",
    "\n",
    "**Game world: game map**\n",
    "\n",
    "Map position: from left to right, from top to bottom, starting counting from 0\n",
    "\n",
    "![](gameworld.png)\n",
    "\n",
    "`log.debug(\"*****game map content: \" + str(vars(game_map))) (snake.py:15)` will give as output:\n",
    "\n",
    "```\n",
    "{\n",
    "   \"game_map\":{\n",
    "      \"width\":46,\n",
    "      \"height\":34,\n",
    "      \"worldTick\":0,\n",
    "      \"snakeInfos\":[\n",
    "         {\n",
    "            \"name\":\"StraightBot\",\n",
    "            \"points\":0,\n",
    "            \"positions\":[\n",
    "               219\n",
    "            ],\n",
    "            \"tailProtectedForGameTicks\":0,\n",
    "            \"id\":\"da72d46b-5727-472d-a901-f5d54e70468e\"\n",
    "         },\n",
    "         {\n",
    "            \"name\":\"ercosnake.py\",\n",
    "            \"points\":0,\n",
    "            \"positions\":[\n",
    "               194\n",
    "            ],\n",
    "            \"tailProtectedForGameTicks\":0,\n",
    "            \"id\":\"afe9ef0d-5904-41fb-b146-cca05471148e\"\n",
    "         },\n",
    "         {\n",
    "            \"name\":\"RandomBot\",\n",
    "            \"points\":0,\n",
    "            \"positions\":[\n",
    "               1495\n",
    "            ],\n",
    "            \"tailProtectedForGameTicks\":0,\n",
    "            \"id\":\"d14ea681-7290-4bb9-8f79-843bf2bf5736\"\n",
    "         },\n",
    "         {\n",
    "            \"name\":\"Snakey\",\n",
    "            \"points\":0,\n",
    "            \"positions\":[\n",
    "               1008\n",
    "            ],\n",
    "            \"tailProtectedForGameTicks\":0,\n",
    "            \"id\":\"ca3e65ba-5633-4478-8f91-b09989d46f52\"\n",
    "         },\n",
    "         {\n",
    "            \"name\":\"StraightBot\",\n",
    "            \"points\":0,\n",
    "            \"positions\":[\n",
    "               969\n",
    "            ],\n",
    "            \"tailProtectedForGameTicks\":0,\n",
    "            \"id\":\"777ec222-3959-484e-80ac-69907f5f1c9e\"\n",
    "         }\n",
    "      ],\n",
    "      \"foodPositions\":[\n",
    "      ],\n",
    "      \"obstaclePositions\":[\n",
    "         70,\n",
    "         450,\n",
    "         451,\n",
    "         496,\n",
    "         497,\n",
    "         1360,\n",
    "         1361,\n",
    "         1362,\n",
    "         1406,\n",
    "         1407,\n",
    "         1408,\n",
    "         1430,\n",
    "         1452,\n",
    "         1453,\n",
    "         1454,\n",
    "         1532\n",
    "      ]\n",
    "   },\n",
    "   \"width\":46,\n",
    "   \"height\":34\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "**Game world: default game rules**\n",
    "\n",
    "* Snake grows every third game tick\n",
    "* Each client must respond within 250ms\n",
    "* 1 point per Snake growth\n",
    "* 2 points per star consumed\n",
    "* 10 points per tail nibble\n",
    "* 5 points per caused death (another snake crashes and dies into your snake)\n",
    "* 5 black holes\n",
    "* A nibbled tail is protected for 3 game ticks\n",
    "* **The last surviving Snake always wins. The ranking for dead snakes is based on accumulated points.**\n",
    "\n",
    "\n",
    "**Game world: game settings**\n",
    "\n",
    "`log.debug('*****player_registered: %s', msg) (client.py: 116)` will give as output the game setting:\n",
    "```\n",
    "{\n",
    "   \"type\":\"se.cygni.snake.api.response.PlayerRegistered\",\n",
    "   \"gameId\":\"b2eaa2de-2a04-4f71-9e31-7f9299dcd92a\",\n",
    "   \"name\":\"ercosnake.py\",\n",
    "   \"gameSettings\":{\n",
    "      \"maxNoofPlayers\":5,\n",
    "      \"startSnakeLength\":1,\n",
    "      \"timeInMsPerTick\":250,\n",
    "      \"obstaclesEnabled\":True,\n",
    "      \"foodEnabled\":True,\n",
    "      \"headToTailConsumes\":True,\n",
    "      \"tailConsumeGrows\":False,\n",
    "      \"addFoodLikelihood\":15,\n",
    "      \"removeFoodLikelihood\":5,\n",
    "      \"spontaneousGrowthEveryNWorldTick\":3,\n",
    "      \"trainingGame\":False,\n",
    "      \"pointsPerLength\":1,\n",
    "      \"pointsPerFood\":2,\n",
    "      \"pointsPerCausedDeath\":5,\n",
    "      \"pointsPerNibble\":10,\n",
    "      \"noofRoundsTailProtectedAfterNibble\":3,\n",
    "      \"startFood\":0,\n",
    "      \"startObstacles\":5\n",
    "   },\n",
    "   \"gameMode\":\"TRAINING\",\n",
    "   \"receivingPlayerId\":\"4165f2d1-0114-486c-ab45-ec0e522c0c47\",\n",
    "   \"timestamp\":1569673113458\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "**Snake client and ML**\n",
    "* Heavily training the client via the network might be problematic -> you might decide to run the server locally.\n",
    "* The code has clearly been written by software engineers. Some rewriting of the snake client is needed to make it usable for ML-training. \n",
    "* A way to run the snake multiple times is to change the following code in `client.py`:\n",
    "\n",
    "```\n",
    " factory = WebSocketClientFactory(u\"ws://%s:%s/%s\" % (args.host, args.port, args.venue))\n",
    " factory.protocol = SnakebotProtocol\n",
    " coro = loop.create_connection(factory, args.host, args.port)\n",
    " loop.run_until_complete(coro)\n",
    " loop.run_forever()\n",
    " loop.close()\n",
    " sys.exit(0)\n",
    " ```\n",
    "  \n",
    "  to\n",
    "\n",
    "```\n",
    "factory = WebSocketClientFactory(u\"ws://%s:%s/%s\" % (args.host, args.port, args.venue))\n",
    "    for i in range(nr_of_episodes):\n",
    "        factory.protocol = SnakebotProtocol\n",
    "        coro = loop.create_connection(factory, args.host, args.port)\n",
    "        loop.run_until_complete(coro)\n",
    "        loop.run_forever()\n",
    "    loop.close()\n",
    "    sys.exit(0)\n",
    "```\n",
    "  \n",
    "* The cygni code is such that the communication protocol class opens a state machine class (== the snake). This is not suitable for ML training. To change this, we can make the snake a static variable that is shared between instances of the communication protocol class. In python you can create a static variable as follows:\n",
    "\n",
    "```\n",
    "# python equivalent of static variable\n",
    "class Snake:\n",
    "    def __init__(self):\n",
    "            self.name = \"\"\n",
    "\n",
    "class SnakebotProtocol:\n",
    "    static_snake = Snake()\n",
    "    \n",
    "    def get_snake_name(self):\n",
    "        return self.static_snake.name\n",
    "        \n",
    "    def set_snake_name(self, name):\n",
    "        self.static_snake.name = name        \n",
    "\n",
    "snakebot_protocol1 = SnakebotProtocol()\n",
    "snakebot_protocol1.set_snake_name(\"dqn_snake\")\n",
    "snakebot_protocol2 = SnakebotProtocol()\n",
    "print(snakebot_protocol2.get_snake_name())\n",
    "```\n",
    "\n",
    "[Based on](https://github.com/cygni/snakebot/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
